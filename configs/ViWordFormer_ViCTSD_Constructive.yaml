vocab:
  type: UIT_ViCTSD_Vocab
  path:
    train: datasets/Dataset/UIT-ViCTSD/train.json
    dev: datasets/Dataset/UIT-ViCTSD/dev.json
    test: datasets/Dataset/UIT-ViCTSD/test.json
  min_freq: 3
  cls_token: <cls>
  unk_token: <unk>
  pad_token: <pad>

dataset:
  train: 
    type: UIT_ViCTSD_Dataset_Construct
    path: datasets/Dataset/UIT-ViCTSD/train.json
  dev:
    type: UIT_ViCTSD_Dataset_Construct
    path: datasets/Dataset/UIT-ViCTSD/dev.json
  test: 
    type: UIT_ViCTSD_Dataset_Construct
    path: datasets/Dataset/UIT-ViCTSD/test.json
  batch_size: 32
  num_workers: 16

model:
  name: ViWordFormer_Model3layer_ViCTSD_Construct
  architecture: ViWordFormer
  nlayers: 3
  head: 8
  d_model: 512
  hidden_dim : 512
  emb_dim : 512
  d_q: 64
  d_kv: 64
  d_ff: 2048
  output_dim: 2
  dropout: 0.1
  label_smoothing: 0.1
  device: cuda

training:
  checkpoint_path: "checkpoints"
  learning_rate: 0.07
  warmup: 1000
  patience: 5
  score: f1

task: TransformerLabel
