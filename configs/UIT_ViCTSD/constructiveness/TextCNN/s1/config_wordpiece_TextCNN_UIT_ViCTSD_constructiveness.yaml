dataset:
  batch_size: 64
  dev:
    max_len: 256
    path: data/UIT-ViCTSD/dev.json
    type: UIT_ViCTSD_Dataset_Construct
  num_workers: 6
  test:
    max_len: 256
    path: data/UIT-ViCTSD/test.json
    type: UIT_ViCTSD_Dataset_Construct
  train:
    max_len: 256
    path: data/UIT-ViCTSD/train.json
    type: UIT_ViCTSD_Dataset_Construct
model:
  architecture: TextCNN
  device: cuda
  dropout: 0.2
  embedding_dim: 256
  filter_sizes:
  - 3
  - 4
  - 5
  input_dim: 256
  label_smoothing: 0.1
  model_type: ''
  n_filters: 100
  name: TextCNN_Model100filters_UIT_ViCTSD_wordpiece_constructiveness
  num_output: 2
task: TextClassification
training:
  checkpoint_path: checkpoints/UIT_ViCTSD/constructiveness/s1/TextCNN/wordpiece
  learning_rate: 0.1
  patience: 10
  score: f1
  seed: 42
  warmup: 500
vocab:
  bos_id: 1
  bos_piece: <s>
  eos_id: 2
  eos_piece: </s>
  label: constructiveness
  min_freq: 5
  model_prefix: checkpoints/UIT_ViCTSD/constructiveness/s1/TextCNN/wordpiece/UIT_ViCTSD_wordpiece
  model_type: wordpiece
  pad_id: 0
  pad_piece: <pad>
  path:
    dev: data/UIT-ViCTSD/dev.json
    test: data/UIT-ViCTSD/test.json
    train: data/UIT-ViCTSD/train.json
  schema: 1
  space_id: 4
  space_token: <space>
  text: comment
  type: WordPieceTokenizer
  unk_id: 3
  unk_piece: <unk>
  vocab_size: 336
